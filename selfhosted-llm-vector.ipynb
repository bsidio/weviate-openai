{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1537e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45956173",
   "metadata": {},
   "source": [
    "## Demo Flow\n",
    "The demo flow is:\n",
    "- **Prerequisites Setup**: Create a Weaviate instance and install required libraries\n",
    "- **Connect**: Connect to your Weaviate instance \n",
    "- **Schema Configuration**: Configure the schema of your data\n",
    "    - *Note*: Here we can define which OpenAI Embedding Model to use\n",
    "    - *Note*: Here we can configure which properties to index\n",
    "- **Import data**: Load a demo dataset and import it into Weaviate\n",
    "    - *Note*: The import process will automatically index your data - based on the configuration in the schema\n",
    "    - *Note*: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you\n",
    "- **Run Queries**: Query \n",
    "    - *Note*: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you\n",
    "    - *Note*: The `qna-openai` module automatically communicates with the OpenAI completions endpoint\n",
    "\n",
    "Once you've run through this notebook you should have a basic understanding of how to setup and use vector databases for question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a145e",
   "metadata": {},
   "source": [
    "## OpenAI Module in Weaviate\n",
    "All Weaviate instances come equipped with the [text2vec-openai](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) and the [qna-openai](https://weaviate.io/developers/weaviate/modules/reader-generator-modules/qna-openai) modules.\n",
    "\n",
    "The first module is responsible for handling vectorization at import (or any CRUD operations) and when you run a search query. The second module communicates with the OpenAI completions endpoint.\n",
    "\n",
    "### No need to manually vectorize data\n",
    "With [text2vec-openai](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.\n",
    "\n",
    "All you need to do is:\n",
    "1. provide your OpenAI API Key – when you connected to the Weaviate Client\n",
    "2. define which OpenAI vectorizer to use in your Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a618c5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start this project, we need setup the following:\n",
    "\n",
    "* create a `Weaviate` instance\n",
    "* install libraries\n",
    "    * `weaviate-client`\n",
    "    * `datasets`\n",
    "    * `apache-beam`\n",
    "* get your [OpenAI API key](https://beta.openai.com/account/api-keys)\n",
    "\n",
    "===========================================================\n",
    "### Create a Weaviate instance\n",
    "\n",
    "To create a Weaviate instance we have 2 options:\n",
    "\n",
    "1. (Recommended path) [Weaviate Cloud Service](https://console.weaviate.io/) – to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.\n",
    "2. Install and run Weaviate locally with Docker.\n",
    "\n",
    "#### Option 1 – WCS Installation Steps\n",
    "\n",
    "Use [Weaviate Cloud Service](https://console.weaviate.io/) (WCS) to create a free Weaviate cluster.\n",
    "1. create a free account and/or login to [WCS](https://console.weaviate.io/)\n",
    "2. create a `Weaviate Cluster` with the following settings:\n",
    "    * Sandbox: `Sandbox Free`\n",
    "    * Weaviate Version: Use default (latest)\n",
    "    * OIDC Authentication: `Disabled`\n",
    "3. your instance should be ready in a minute or two\n",
    "4. make a note of the `Cluster Id`. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: `https://your-project-name.weaviate.network` \n",
    "\n",
    "#### Option 2 – local Weaviate instance with Docker\n",
    "\n",
    "Install and run Weaviate locally with Docker.\n",
    "1. Download the [./docker-compose.yml](./docker-compose.yml) file\n",
    "2. Then open your terminal, navigate to where your docker-compose.yml file is located, and start docker with: `docker-compose up -d`\n",
    "3. Once this is ready, your instance should be available at [http://localhost:8080](http://localhost:8080)\n",
    "\n",
    "Note. To shut down your docker instance you can call: `docker-compose down`\n",
    "\n",
    "##### Learn more\n",
    "To learn more, about using Weaviate with Docker see the [installation documentation](https://weaviate.io/developers/weaviate/installation/docker-compose)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9babafe",
   "metadata": {},
   "source": [
    "===========================================================    \n",
    "## Install required libraries\n",
    "\n",
    "Before running this project make sure to have the following libraries:\n",
    "\n",
    "### Weaviate Python client\n",
    "\n",
    "The [Weaviate Python client](https://weaviate.io/developers/weaviate/client-libraries/python) allows you to communicate with your Weaviate instance from your Python project.\n",
    "\n",
    "### datasets & apache-beam\n",
    "\n",
    "To load sample data, you need the `datasets` library and its' dependency `apache-beam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b04113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[K     |████████████████████████████████| 542 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting apache-beam\n",
      "  Downloading apache-beam-2.55.1.tar.gz (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 396 kB/s eta 0:00:01\n",
      "\u001b[?25h\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-92n10m8c/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-92n10m8c/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-92n10m8c/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-92n10m8c/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-92n10m8c/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-92n10m8c/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting fsspec[http]<=2024.3.1,>=2023.1.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /Users/sb/Library/Python/3.9/lib/python/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.1)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from datasets) (4.66.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from datasets) (2.31.0)\n",
      "Collecting huggingface-hub>=0.21.2\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[K     |████████████████████████████████| 388 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-16.0.0-cp39-cp39-macosx_11_0_arm64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/sb/Library/Python/3.9/lib/python/site-packages (from datasets) (23.2)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting objsize<0.8.0,>=0.6.1\n",
      "  Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
      "Collecting zstandard<1,>=0.18.0\n",
      "  Downloading zstandard-0.22.0-cp39-cp39-macosx_11_0_arm64.whl (703 kB)\n",
      "\u001b[K     |████████████████████████████████| 703 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting js2py<1,>=0.74\n",
      "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from apache-beam) (1.62.2)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from apache-beam) (4.25.3)\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.9.4.tar.gz (985 kB)\n",
      "\u001b[K     |████████████████████████████████| 985 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25h\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-1wc1xqgc/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-1wc1xqgc/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-1wc1xqgc/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-1wc1xqgc/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-1wc1xqgc/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/gs/7cbp7gqn71s1v401j22p1xlh0000gn/T/pip-build-env-1wc1xqgc/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: orjson<4,>=3.9.7 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from apache-beam) (3.10.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2018.3\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[K     |████████████████████████████████| 505 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema<5.0.0,>=4.0.0\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 4.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.0 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from apache-beam) (4.11.0)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-14.0.2-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2020.6.8\n",
      "  Downloading regex-2024.4.16-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pymongo<5.0.0,>=3.8.0\n",
      "  Downloading pymongo-4.6.3-cp39-cp39-macosx_10_9_universal2.whl (534 kB)\n",
      "\u001b[K     |████████████████████████████████| 534 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httplib2<0.23.0,>=0.8\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from apache-beam) (2.9.0.post0)\n",
      "Collecting fasteners<1.0,>=0.3\n",
      "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0\n",
      "  Downloading jsonpickle-3.0.4-py3-none-any.whl (39 kB)\n",
      "Collecting cloudpickle~=2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 8.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.15.0)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzlocal>=1.2\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting pyjsparser>=2.5.1\n",
      "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.34.0-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.18.0-cp39-cp39-macosx_11_0_arm64.whl (330 kB)\n",
      "\u001b[K     |████████████████████████████████| 330 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[K     |████████████████████████████████| 307 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sb/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.12.1.zip (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.12-py39-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.11.1-py39-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.11-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.10.zip (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading multiprocess-0.70.9.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[K     |████████████████████████████████| 345 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: apache-beam, crcmod, dill, fastavro, hdfs, pyjsparser, docopt, multiprocess\n",
      "  Building wheel for apache-beam (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for apache-beam: filename=apache_beam-2.55.1-cp39-cp39-macosx_10_9_universal2.whl size=7118171 sha256=6ee788ffd05737b5ece73511f25e318e7e39ebdc544b19f51e62747073b950cd\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/9a/c0/ca/e9cf7b64cbb691e07dd05d6c3acb444776606f93844a1b04ce\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-macosx_10_9_universal2.whl size=25216 sha256=fc3a285d7c43fea1c5e06d9d0916e78c15ff04687c29828e06a51422eb02ad03\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=98aa27d255c47edda070b80df2fd7092b3cacbc7b59b9ba21b8f533a81e918e7\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for fastavro (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastavro: filename=fastavro-1.9.4-cp39-cp39-macosx_10_9_universal2.whl size=1036346 sha256=e81ccd4b032a6abdadf4da6ca71d192cda98c634bf15401ee1344630ea86cb73\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/fc/b1/73/b01eaf80e7465e17b65c335a32e4c5992247bcefd2c396e2b9\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34347 sha256=28ea8462d69f9cac3e569ef1a4452f44b960742aec7665dfcbddfdf10877d06e\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/05/6f/21/aa8d233f90da3017b4ef7c61829589dc267402d376dd3efcf5\n",
      "  Building wheel for pyjsparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=26001 sha256=fc23d75f9ee672ae8f3de797d7ee70181c0bc6bfcf7e1519a1e75651af82eb92\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/f0/70/61/f42dc45dcf0fbe8c495ce579b04730787081499bfb5b8bc60e\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=525c8a57727bc9b74ccc0ef59c07445fd2fdaf2deb4e17eb79cc56e9663e9e91\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for multiprocess (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multiprocess: filename=multiprocess-0.70.9-py3-none-any.whl size=125283 sha256=6ab51cd3943e5a62ab5cc3eea0745693c52060a8e7954064ae142dc2e5a524eb\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/42/11/d8/b959e3291132f362f07c763e23b9f581b74cd6e4962ed73b8d\n",
      "Successfully built apache-beam crcmod dill fastavro hdfs pyjsparser docopt multiprocess\n",
      "Installing collected packages: rpds-py, referencing, tzlocal, tzdata, pytz, pyparsing, pyjsparser, jsonschema-specifications, fsspec, filelock, docopt, dnspython, dill, zstandard, xxhash, regex, pymongo, pydot, pyarrow-hotfix, pyarrow, proto-plus, pandas, objsize, multiprocess, jsonschema, jsonpickle, js2py, huggingface-hub, httplib2, hdfs, fasteners, fastavro, crcmod, cloudpickle, datasets, apache-beam\n",
      "Successfully installed apache-beam-2.55.1 cloudpickle-2.2.1 crcmod-1.7 datasets-2.19.0 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 filelock-3.13.4 fsspec-2024.3.1 hdfs-2.7.3 httplib2-0.22.0 huggingface-hub-0.22.2 js2py-0.74 jsonpickle-3.0.4 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 multiprocess-0.70.9 objsize-0.7.0 pandas-2.2.2 proto-plus-1.23.0 pyarrow-14.0.2 pyarrow-hotfix-0.6 pydot-1.4.2 pyjsparser-2.7.1 pymongo-4.6.3 pyparsing-3.1.2 pytz-2024.1 referencing-0.34.0 regex-2024.4.16 rpds-py-0.18.0 tzdata-2024.1 tzlocal-5.2 xxhash-3.4.1 zstandard-0.22.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the Weaviate client for Python\n",
    "!pip3 install weaviate-client>3.11.0\n",
    "\n",
    "# Install datasets and apache-beam to load the sample datasets\n",
    "!pip3 install datasets apache-beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe86f4",
   "metadata": {},
   "source": [
    "===========================================================\n",
    "## Prepare your OpenAI API key\n",
    "\n",
    "The `OpenAI API key` is used for vectorization of your data at import, and for queries.\n",
    "\n",
    "If you don't have an OpenAI API key, you can get one from [https://beta.openai.com/account/api-keys](https://beta.openai.com/account/api-keys).\n",
    "\n",
    "Once you get your key, please add it to your environment variables as `OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2ded4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export OpenAI API Key\n",
    "!export OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88be138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "# Test that your OpenAI API key is correctly set as an environment variable\n",
    "# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n",
    "import os\n",
    "\n",
    "# Note. alternatively you can set a temporary env variable like this:\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df4d5b",
   "metadata": {},
   "source": [
    "## Connect to your Weaviate instance\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. test env variable `OPENAI_API_KEY` – **make sure** you completed the step in [#Prepare-your-OpenAI-API-key](#Prepare-your-OpenAI-API-key)\n",
    "2. connect to your Weaviate your `OpenAI API Key`\n",
    "3. and test the client connection\n",
    "\n",
    "### The client \n",
    "\n",
    "After this step, the `client` object will be used to perform all Weaviate-related operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc662c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sb/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sb/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sb/Library/Python/3.9/lib/python/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weaviate\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Connect to your Weaviate instance\n",
    "client = weaviate.Client(\n",
    "  #  url=\"https://your-wcs-instance-name.weaviate.network/\",\n",
    "   url=\"https://vectordb.bsid.io/\",\n",
    "   # auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"<YOUR-WEAVIATE-API-KEY>\"), # comment out this line if you are not using authentication for your Weaviate instance (i.e. for locally deployed instances)\n",
    "    additional_headers={\n",
    "        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check if your instance is live and ready\n",
    "# This should return `True`\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dac3c",
   "metadata": {},
   "source": [
    "# Schema\n",
    "\n",
    "In this section, we will:\n",
    "1. configure the data schema for your data\n",
    "2. select OpenAI module\n",
    "\n",
    "> This is the second and final step, which requires OpenAI specific configuration.\n",
    "> After this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.\n",
    "\n",
    "\n",
    "## What is a schema\n",
    "\n",
    "In Weaviate you create __schemas__ to capture each of the entities you will be searching.\n",
    "\n",
    "A schema is how you tell Weaviate:\n",
    "* what embedding model should be used to vectorize the data\n",
    "* what your data is made of (property names and types)\n",
    "* which properties should be vectorized and indexed\n",
    "\n",
    "In this cookbook we will use a dataset for `Articles`, which contains:\n",
    "* `title`\n",
    "* `content`\n",
    "* `url`\n",
    "\n",
    "We want to vectorize `title` and `content`, but not the `url`.\n",
    "\n",
    "To vectorize and query the data, we will use `text-embedding-3-small`. For Q&A we will use `gpt-3.5-turbo-instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f894b911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [{'class': 'Article',\n",
       "   'description': 'A collection of articles',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'qna-openai': {'frequencyPenalty': 0.0,\n",
       "     'maxTokens': 16,\n",
       "     'model': 'gpt-3.5-turbo-instruct',\n",
       "     'presencePenalty': 0.0,\n",
       "     'temperature': 0.0,\n",
       "     'topP': 1},\n",
       "    'text2vec-openai': {'baseURL': 'https://api.openai.com',\n",
       "     'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'Title of the article',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'title',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'Contents of the article',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'content',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'URL to the article',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': True,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'url',\n",
       "     'tokenization': 'whitespace'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear up the schema, so that we can recreate it\n",
    "client.schema.delete_all()\n",
    "client.schema.get()\n",
    "\n",
    "# Define the Schema object to use `text-embedding-3-small` on `title` and `content`, but skip it for `url`\n",
    "article_schema = {\n",
    "    \"class\": \"Article\",\n",
    "    \"description\": \"A collection of articles\",\n",
    "    \"vectorizer\": \"text2vec-openai\",\n",
    "    \"moduleConfig\": {\n",
    "        \"text2vec-openai\": {\n",
    "          \"model\": \"ada\",\n",
    "          \"modelVersion\": \"002\",\n",
    "          \"type\": \"text\"\n",
    "        }, \n",
    "        \"qna-openai\": {\n",
    "          \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "          \"maxTokens\": 16,\n",
    "          \"temperature\": 0.0,\n",
    "          \"topP\": 1,\n",
    "          \"frequencyPenalty\": 0.0,\n",
    "          \"presencePenalty\": 0.0\n",
    "        }\n",
    "    },\n",
    "    \"properties\": [{\n",
    "        \"name\": \"title\",\n",
    "        \"description\": \"Title of the article\",\n",
    "        \"dataType\": [\"string\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"content\",\n",
    "        \"description\": \"Contents of the article\",\n",
    "        \"dataType\": [\"text\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"url\",\n",
    "        \"description\": \"URL to the article\",\n",
    "        \"dataType\": [\"string\"],\n",
    "        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n",
    "    }]\n",
    "}\n",
    "\n",
    "# add the Article schema\n",
    "client.schema.create_class(article_schema)\n",
    "\n",
    "# get the schema to make sure it worked\n",
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9d2e1",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "In this section we will:\n",
    "1. load the Simple Wikipedia dataset\n",
    "2. configure Weaviate Batch import (to make the import more efficient)\n",
    "3. import the data into Weaviate\n",
    "\n",
    "> Note: <br/>\n",
    "> Like mentioned before. We don't need to manually vectorize the data.<br/>\n",
    "> The [text2vec-openai](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) module will take care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3efadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sb/Library/Python/3.9/lib/python/site-packages/datasets/load.py:1486: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 36.7k/36.7k [00:00<00:00, 4.42MB/s]\n",
      "Downloading readme: 100%|██████████| 16.0k/16.0k [00:00<00:00, 19.6MB/s]\n",
      "Downloading data: 100%|██████████| 134M/134M [00:15<00:00, 8.56MB/s] \n",
      "Generating train split: 100%|██████████| 205328/205328 [00:00<00:00, 775976.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### STEP 1 - load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from typing import List, Iterator\n",
    "\n",
    "# We'll use the datasets library to pull the Simple Wikipedia dataset for embedding\n",
    "dataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])\n",
    "\n",
    "# For testing, limited to 2.5k articles for demo purposes\n",
    "dataset = dataset[:2_500]\n",
    "\n",
    "# Limited to 25k articles for larger demo purposes\n",
    "# dataset = dataset[:25_000]\n",
    "\n",
    "# for free OpenAI acounts, you can use 50 objects\n",
    "# dataset = dataset[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5044da96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.batch.crud_batch.Batch at 0x11eda1340>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 2 - configure Weaviate Batch, with\n",
    "# - starting batch size of 100\n",
    "# - dynamically increase/decrease based on performance\n",
    "# - add timeout retries if something goes wrong\n",
    "\n",
    "client.batch.configure(\n",
    "    batch_size=10, \n",
    "    dynamic=True,\n",
    "    timeout_retries=3,\n",
    "#   callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15db8380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Articles\n",
      "Import 0 / 2500 \n",
      "Import 10 / 2500 \n",
      "Import 20 / 2500 \n",
      "Import 30 / 2500 \n",
      "Import 40 / 2500 \n",
      "Import 50 / 2500 \n",
      "Import 60 / 2500 \n",
      "Import 70 / 2500 \n",
      "Import 80 / 2500 \n",
      "Import 90 / 2500 \n",
      "Import 100 / 2500 \n",
      "Import 110 / 2500 \n",
      "Import 120 / 2500 \n",
      "Import 130 / 2500 \n",
      "Import 140 / 2500 \n",
      "Import 150 / 2500 \n",
      "Import 160 / 2500 \n",
      "Import 170 / 2500 \n",
      "Import 180 / 2500 \n",
      "Import 190 / 2500 \n",
      "Import 200 / 2500 \n",
      "Import 210 / 2500 \n",
      "Import 220 / 2500 \n",
      "Import 230 / 2500 \n",
      "Import 240 / 2500 \n",
      "Import 250 / 2500 \n",
      "Import 260 / 2500 \n",
      "Import 270 / 2500 \n",
      "Import 280 / 2500 \n",
      "Import 290 / 2500 \n",
      "Import 300 / 2500 \n",
      "Import 310 / 2500 \n",
      "Import 320 / 2500 \n",
      "Import 330 / 2500 \n",
      "Import 340 / 2500 \n",
      "Import 350 / 2500 \n",
      "Import 360 / 2500 \n",
      "Import 370 / 2500 \n",
      "Import 380 / 2500 \n",
      "Import 390 / 2500 \n",
      "Import 400 / 2500 \n",
      "Import 410 / 2500 \n",
      "Import 420 / 2500 \n",
      "Import 430 / 2500 \n",
      "Import 440 / 2500 \n",
      "Import 450 / 2500 \n",
      "Import 460 / 2500 \n",
      "Import 470 / 2500 \n",
      "Import 480 / 2500 \n",
      "Import 490 / 2500 \n",
      "Import 500 / 2500 \n",
      "Import 510 / 2500 \n",
      "Import 520 / 2500 \n",
      "Import 530 / 2500 \n",
      "Import 540 / 2500 \n",
      "Import 550 / 2500 \n",
      "Import 560 / 2500 \n",
      "Import 570 / 2500 \n",
      "Import 580 / 2500 \n",
      "Import 590 / 2500 \n",
      "Import 600 / 2500 \n",
      "Import 610 / 2500 \n",
      "Import 620 / 2500 \n",
      "Import 630 / 2500 \n",
      "Import 640 / 2500 \n",
      "Import 650 / 2500 \n",
      "Import 660 / 2500 \n",
      "Import 670 / 2500 \n",
      "Import 680 / 2500 \n",
      "Import 690 / 2500 \n",
      "Import 700 / 2500 \n",
      "Import 710 / 2500 \n",
      "Import 720 / 2500 \n",
      "Import 730 / 2500 \n",
      "Import 740 / 2500 \n",
      "Import 750 / 2500 \n",
      "Import 760 / 2500 \n",
      "Import 770 / 2500 \n",
      "{'error': [{'message': \"update vector: connection to: OpenAI API failed with status: 400 error: This model's maximum context length is 8192 tokens, however you requested 8781 tokens (8781 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\"}]}\n",
      "Import 780 / 2500 \n",
      "Import 790 / 2500 \n",
      "Import 800 / 2500 \n",
      "Import 810 / 2500 \n",
      "Import 820 / 2500 \n",
      "Import 830 / 2500 \n",
      "Import 840 / 2500 \n",
      "Import 850 / 2500 \n",
      "Import 860 / 2500 \n",
      "Import 870 / 2500 \n",
      "Import 880 / 2500 \n",
      "Import 890 / 2500 \n",
      "Import 900 / 2500 \n",
      "Import 910 / 2500 \n",
      "Import 920 / 2500 \n",
      "Import 930 / 2500 \n",
      "{'error': [{'message': \"update vector: connection to: OpenAI API failed with status: 400 error: This model's maximum context length is 8192 tokens, however you requested 11581 tokens (11581 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\"}]}\n",
      "Import 940 / 2500 \n",
      "Import 950 / 2500 \n",
      "Import 960 / 2500 \n",
      "Import 970 / 2500 \n",
      "Import 980 / 2500 \n",
      "Import 990 / 2500 \n",
      "Import 1000 / 2500 \n",
      "Import 1010 / 2500 \n",
      "Import 1020 / 2500 \n",
      "Import 1030 / 2500 \n",
      "{'error': [{'message': \"update vector: connection to: OpenAI API failed with status: 400 error: This model's maximum context length is 8192 tokens, however you requested 10049 tokens (10049 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\"}]}\n",
      "Import 1040 / 2500 \n",
      "Import 1050 / 2500 \n",
      "Import 1060 / 2500 \n",
      "Import 1070 / 2500 \n",
      "Import 1080 / 2500 \n",
      "{'error': [{'message': \"update vector: connection to: OpenAI API failed with status: 400 error: This model's maximum context length is 8192 tokens, however you requested 13057 tokens (13057 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\"}]}\n",
      "Import 1090 / 2500 \n",
      "Import 1100 / 2500 \n",
      "Import 1110 / 2500 \n",
      "Import 1120 / 2500 \n",
      "Import 1130 / 2500 \n",
      "Import 1140 / 2500 \n",
      "Import 1150 / 2500 \n",
      "Import 1160 / 2500 \n",
      "Import 1170 / 2500 \n",
      "{'error': [{'message': \"update vector: connection to: OpenAI API failed with status: 400 error: This model's maximum context length is 8192 tokens, however you requested 8411 tokens (8411 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\"}]}\n",
      "Import 1180 / 2500 \n",
      "Import 1190 / 2500 \n",
      "Import 1200 / 2500 \n",
      "Import 1210 / 2500 \n",
      "Import 1220 / 2500 \n",
      "Import 1230 / 2500 \n",
      "Import 1240 / 2500 \n",
      "Import 1250 / 2500 \n",
      "Import 1260 / 2500 \n",
      "{'error': [{'message': \"update vector: connection to: OpenAI API failed with status: 400 error: This model's maximum context length is 8192 tokens, however you requested 11585 tokens (11585 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\"}]}\n",
      "Import 1270 / 2500 \n",
      "Import 1280 / 2500 \n",
      "Import 1290 / 2500 \n",
      "Import 1300 / 2500 \n",
      "Import 1310 / 2500 \n",
      "Import 1320 / 2500 \n",
      "Import 1330 / 2500 \n",
      "Import 1340 / 2500 \n",
      "Import 1350 / 2500 \n",
      "Import 1360 / 2500 \n",
      "Import 1370 / 2500 \n",
      "Import 1380 / 2500 \n",
      "Import 1390 / 2500 \n",
      "Import 1400 / 2500 \n",
      "Import 1410 / 2500 \n",
      "Import 1420 / 2500 \n",
      "Import 1430 / 2500 \n",
      "Import 1440 / 2500 \n",
      "Import 1450 / 2500 \n",
      "Import 1460 / 2500 \n",
      "Import 1470 / 2500 \n",
      "Import 1480 / 2500 \n",
      "Import 1490 / 2500 \n",
      "Import 1500 / 2500 \n",
      "Import 1510 / 2500 \n",
      "Import 1520 / 2500 \n",
      "Import 1530 / 2500 \n",
      "Import 1540 / 2500 \n",
      "Import 1550 / 2500 \n",
      "Import 1560 / 2500 \n",
      "Import 1570 / 2500 \n",
      "Import 1580 / 2500 \n",
      "Import 1590 / 2500 \n",
      "Import 1600 / 2500 \n",
      "Import 1610 / 2500 \n",
      "Import 1620 / 2500 \n",
      "Import 1630 / 2500 \n",
      "Import 1640 / 2500 \n",
      "Import 1650 / 2500 \n",
      "Import 1660 / 2500 \n",
      "Import 1670 / 2500 \n",
      "Import 1680 / 2500 \n",
      "Import 1690 / 2500 \n",
      "Import 1700 / 2500 \n",
      "Import 1710 / 2500 \n",
      "Import 1720 / 2500 \n",
      "Import 1730 / 2500 \n",
      "Import 1740 / 2500 \n",
      "Import 1750 / 2500 \n",
      "Import 1760 / 2500 \n",
      "Import 1770 / 2500 \n",
      "Import 1780 / 2500 \n",
      "Import 1790 / 2500 \n",
      "Import 1800 / 2500 \n",
      "Import 1810 / 2500 \n",
      "Import 1820 / 2500 \n",
      "Import 1830 / 2500 \n",
      "Import 1840 / 2500 \n",
      "Import 1850 / 2500 \n",
      "Import 1860 / 2500 \n",
      "Import 1870 / 2500 \n",
      "Import 1880 / 2500 \n",
      "Import 1890 / 2500 \n",
      "Import 1900 / 2500 \n",
      "Import 1910 / 2500 \n",
      "Import 1920 / 2500 \n",
      "Import 1930 / 2500 \n",
      "Import 1940 / 2500 \n",
      "Import 1950 / 2500 \n",
      "Import 1960 / 2500 \n",
      "Import 1970 / 2500 \n",
      "Import 1980 / 2500 \n",
      "Import 1990 / 2500 \n",
      "Import 2000 / 2500 \n",
      "Import 2010 / 2500 \n",
      "Import 2020 / 2500 \n",
      "Import 2030 / 2500 \n",
      "Import 2040 / 2500 \n",
      "Import 2050 / 2500 \n",
      "Import 2060 / 2500 \n",
      "Import 2070 / 2500 \n",
      "Import 2080 / 2500 \n",
      "Import 2090 / 2500 \n",
      "Import 2100 / 2500 \n",
      "Import 2110 / 2500 \n",
      "Import 2120 / 2500 \n",
      "Import 2130 / 2500 \n",
      "Import 2140 / 2500 \n",
      "Import 2150 / 2500 \n",
      "Import 2160 / 2500 \n",
      "Import 2170 / 2500 \n",
      "Import 2180 / 2500 \n",
      "Import 2190 / 2500 \n",
      "Import 2200 / 2500 \n",
      "Import 2210 / 2500 \n",
      "Import 2220 / 2500 \n",
      "Import 2230 / 2500 \n",
      "Import 2240 / 2500 \n",
      "Import 2250 / 2500 \n",
      "Import 2260 / 2500 \n",
      "Import 2270 / 2500 \n",
      "Import 2280 / 2500 \n",
      "Import 2290 / 2500 \n",
      "Import 2300 / 2500 \n",
      "Import 2310 / 2500 \n",
      "Import 2320 / 2500 \n",
      "Import 2330 / 2500 \n",
      "Import 2340 / 2500 \n",
      "Import 2350 / 2500 \n",
      "Import 2360 / 2500 \n",
      "Import 2370 / 2500 \n",
      "Import 2380 / 2500 \n",
      "Import 2390 / 2500 \n",
      "Import 2400 / 2500 \n",
      "Import 2410 / 2500 \n",
      "Import 2420 / 2500 \n",
      "Import 2430 / 2500 \n",
      "Import 2440 / 2500 \n",
      "Import 2450 / 2500 \n",
      "Import 2460 / 2500 \n",
      "Import 2470 / 2500 \n",
      "Import 2480 / 2500 \n",
      "Import 2490 / 2500 \n",
      "Importing Articles complete\n"
     ]
    }
   ],
   "source": [
    "### Step 3 - import data\n",
    "\n",
    "print(\"Importing Articles\")\n",
    "\n",
    "counter=0\n",
    "\n",
    "with client.batch as batch:\n",
    "    for article in dataset:\n",
    "        if (counter %10 == 0):\n",
    "            print(f\"Import {counter} / {len(dataset)} \")\n",
    "\n",
    "        properties = {\n",
    "            \"title\": article[\"title\"],\n",
    "            \"content\": article[\"text\"],\n",
    "            \"url\": article[\"url\"]\n",
    "        }\n",
    "        \n",
    "        batch.add_data_object(properties, \"Article\")\n",
    "        counter = counter+1\n",
    "\n",
    "print(\"Importing Articles complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3658693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object count:  [{'meta': {'count': 2494}}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test that all data has loaded – get object count\n",
    "result = (\n",
    "    client.query.aggregate(\"Article\")\n",
    "    .with_fields(\"meta { count }\")\n",
    "    .do()\n",
    ")\n",
    "print(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d791186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Sutherland\n",
      "https://simple.wikipedia.org/wiki/Donald%20Sutherland\n",
      "Donald McNichol Sutherland OC (born July 17, 1935) is a Canadian actor. He has appeared in more than 100 movie and television shows.\n",
      "\n",
      "Sutherland is known for his roles in Fellini's Casanova, Klute, Don't Look Now, Invasion of the Body Snatchers, JFK, Ordinary People, Pride & Prejudice, and The Hunger Games. He is the father of actor Kiefer Sutherland.\n",
      "\n",
      "Early life\n",
      "Sutherland was born in Saint John, New Brunswick, Canada. His ancestry includes Scottish, as well as German and English. When Sutherland was a child, he had rheumatic fever, hepatitis and poliomyelitis. He studied at Victoria College and at University of Toronto. He studied acting London Academy of Music and Dramatic Art. Sutherland started off working as a radio DJ at the age of 14.\n",
      "\n",
      "Career\n",
      "\n",
      "Sutherland's acting career began in 1962 with a small role in the television series The Avengers. He then starred in some major roles in movies such as Dr. Terror's House of Horrors with Christopher Lee. He had a recurring role in the 1970 television series M*A*S*H.\n",
      "\n",
      "Sutherland soon gained fame in the Academy Award-winning movie Klute with Jane Fonda. He then starred in the movie Don't Look Now. He then starred in the movie Buffy the Vampire Slayer.\n",
      "\n",
      "Recently, Sutherland portrayed President Snow in the movie adaptation of The Hunger Games, released in March 2012. He repeated the role in its sequel, The Hunger Games: Catching Fire (2013).\n",
      "\n",
      "In 2018, Sutherland played J. Paul Getty in the television series Trust.\n",
      "\n",
      "Personal life\n",
      "Sutherland was married to Lois Hardwick from 1959 until they divorced in 1966. Then he was married to Shirley Douglas from 1966 until they divorced in 1970. Since 1972, Sutherland has been married to his current wife Francine Racette. He has five children, three of whom are actors.\n",
      "\n",
      "Sutherland became a blogger for the American news website The Huffington Post during the 2008 United States presidential election campaign. In his blogs, he openly stated his support for Barack Obama.\n",
      "\n",
      "During the Opening Ceremony of the 2010 Winter Olympics in Vancouver he narrated the events. He was also one of the Olympic flag bearers.\n",
      "\n",
      "Movies\n",
      "\n",
      " Castle of the Living Dead (1964)\n",
      " Dr. Terror's House of Horrors (1965)\n",
      " The Dirty Dozen (1967)\n",
      " Interlude (1968)\n",
      " Start the Revolution Without Me (1970)\n",
      " M*A*S*H (1970)\n",
      " Klute (1971)\n",
      " Lady Ice (1973)\n",
      " S*P*Y*S (1974)\n",
      " The Kentucky Fried Movie (1977)\n",
      " National Lampoon's Animal House (1978)\n",
      " The Great Train Robbery (1979)\n",
      " Bear Island (1979)\n",
      " Eye of the Needle (1981)\n",
      " Scream of Stone (1991)\n",
      " Long Road Home (1991)\n",
      " JFK (1991)\n",
      " Eminent Domain (1991)\n",
      " Backdraft (1991)\n",
      " Quicksand: No Escape (1992)\n",
      " Buffy the Vampire Slayer (1992)\n",
      " Benefit of the Doubt (1993)\n",
      " The Puppet Masters (1994)\n",
      " Punch (1994)\n",
      " Oldest Living Confederate Widow Tells All (1994) \n",
      " The Lifeforce Experiment (1994)\n",
      " Disclosure (1994)\n",
      " Behind the Mask (1999) \n",
      " Virus (1999)\n",
      " Instinct (1999)\n",
      " The Hunley (1999)\n",
      " Toscano (1999)\n",
      " The Setting Sun (1999)\n",
      " Panic (2000)\n",
      " Space Cowboys (2000)\n",
      " The Art of War (2000)\n",
      " Threads of Hope (voice) (2000)\n",
      " Final Fantasy: The Spirits Within (voice) (2001)\n",
      " Uprising (TV) (2001)\n",
      " Frankenstein (TV) (2004)\n",
      " Aurora Borealis (2004)\n",
      " The Hunger Games (2012)\n",
      " The Hunger Games: Catching Fire (2013)\n",
      " Jappeloup (2013)\n",
      " The Hunger Games: Mockingjay – Part 1 (2014)\n",
      " The Hunger Games: Mockingjay – Part 2 (2015)\n",
      " Trust (2018)\n",
      "\n",
      "Awards\n",
      "\n",
      " 1978: Officer of the Order of Canada (OC)\n",
      " 1983: 4th Genie Awards, winner, Best Actor, Threshold\n",
      " 1995: Primetime Emmy Award for Outstanding Actor in a Supporting Role in a Miniseries or a Movie, Citizen X\n",
      " 1995: Golden Globe Award, winner, Best Supporting Actor – Series, Miniseries or Television Film, Citizen X\n",
      " 1998: Satellite Award for Best Supporting Actor – Drama, Without Limits\n",
      " 2000: Canada's Walk of Fame\n",
      " 2002: Golden Globe Award, winner, Best Supporting Actor – Series, Miniseries or Television Film, Path to War\n",
      " 2005: Honorary Doctor of Arts (Hon DArt) from Middlebury College (Middlebury, Vermont, US)\n",
      " 2011: Hollywood Walk Of Fame.\n",
      " 2012: Commandeur of the Ordre des Arts et des Lettres\n",
      "\n",
      "References\n",
      "\n",
      "Other websites \n",
      "\n",
      " On the Money (Carole Cadwalladr interview), The Guardian, 30 March 2008\n",
      "\n",
      "1935 births\n",
      "Living people\n",
      "Academy Award Honorary Award winners\n",
      "Canadian movie actors\n",
      "Canadian movie producers\n",
      "Canadian stage actors\n",
      "Canadian television actors\n",
      "Canadian voice actors\n",
      "Emmy Award winning actors\n",
      "Golden Globe Award winning actors\n",
      "Order of Canada\n",
      "People from New Brunswick\n",
      "Satellite Award winners\n"
     ]
    }
   ],
   "source": [
    "# Test one article has worked by checking one object\n",
    "test_article = (\n",
    "    client.query\n",
    "    .get(\"Article\", [\"title\", \"url\", \"content\"])\n",
    "    .with_limit(1)\n",
    "    .do()\n",
    ")[\"data\"][\"Get\"][\"Article\"][0]\n",
    "\n",
    "print(test_article['title'])\n",
    "print(test_article['url'])\n",
    "print(test_article['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46050ca9",
   "metadata": {},
   "source": [
    "### Question Answering on the Data\n",
    "\n",
    "As above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b044aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qna(queryText, collection_name):\n",
    "    \n",
    "    properties = [\n",
    "        \"title\", \"content\", \"url\",\n",
    "        \"_additional { answer { hasAnswer property result startPosition endPosition } distance }\"\n",
    "    ]\n",
    "\n",
    "    ask = {\n",
    "        \"question\": queryText,\n",
    "        \"properties\": [\"content\"]\n",
    "    }\n",
    "\n",
    "    result = (\n",
    "        client.query\n",
    "        .get(collection_name, properties)\n",
    "        .with_ask(ask)\n",
    "        .with_limit(1)\n",
    "        .do()\n",
    "    )\n",
    "    \n",
    "    # Check for errors\n",
    "    if (\"errors\" in result):\n",
    "        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute – the limit is set at 60 per minute.\")\n",
    "        raise Exception(result[\"errors\"][0]['message'])\n",
    "    print(result[\"data\"][\"Get\"][collection_name])\n",
    "    return result[\"data\"][\"Get\"][collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2025f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_additional': {'answer': {'endPosition': 0, 'hasAnswer': True, 'property': '', 'result': ' Yes, Alanis Morissette won four Grammy Awards for her album Jagged', 'startPosition': 0}, 'distance': 0.157219}, 'content': 'Alanis Nadine Morissette (born June 1, 1974) is a Grammy Award-winning Canadian-American singer and songwriter. She was born in Ottawa, Canada. She began singing in Canada as a teenager in 1990. In 1995, she became popular all over the world.\\n\\nAs a young child in Canada, Morissette began to act on television, including 5 episodes of the long-running series, You Can\\'t Do That on Television. Her first album was released only in Canada in 1990.\\n\\nHer first international album was Jagged Little Pill, released in 1995. It was a rock-influenced album. Jagged has sold more than 33 million units globally. It became the best-selling debut album in music history. Her next album, Supposed Former Infatuation Junkie, was released in 1998. It was a success as well. Morissette took up producing duties for her next albums, which include Under Rug Swept, So-Called Chaos and Flavors of Entanglement. Morissette has sold more than 60 million albums worldwide.\\n\\nShe also acted in several movies, including Kevin Smith\\'s Dogma, where she played God.\\n\\nAbout her life\\nAlanis Morissette was born in Riverside Hospital of Ottawa in Ottawa, Ontario. Her father is French-Canadian. Her mother is from Hungary. She has an older brother, Chad, and a twin brother, Wade, who is 12 minutes younger than she is. Her parents had worked as teachers at a military base in Lahr, Germany.\\n\\nMorissette became an American citizen in 2005. She is still Canadian citizen.\\n\\nOn May 22, 2010, Morissette married rapper Mario \"MC Souleye\" Treadway.\\n\\nJagged Little Pill\\nMorissette has had many albums. Her 1995 album Jagged Little Pill became a very popular album. It has sold over 30 million copies worldwide. The album caused Morissette to win four Grammy Awards. The album Jagged Little Pill touched many people.\\n\\nOn the album, Morissette sang songs about many different things. These things include:\\nlove (in the song \"Head Over Feet\")\\nlife (in the songs \"Ironic\" and \"You Learn\")\\nher feelings (in the songs \"Hand In My Pocket\" and \"All I Really Want\")\\nsadness (in the song \"Mary Jane\")\\nanger (in the song \"You Oughta Know\")\\nfrustration (in the songs \"Not the Doctor\" and \"Wake Up\")\\n\\nDiscography\\n\\nAlbums\\nAlanis (Canada-only, 1991)\\nNow Is the Time (Canada-only, 1992)\\nJagged Little Pill (1995)\\nSupposed Former Infatuation Junkie (1998)\\nAlanis Unplugged (1999)\\nUnder Rug Swept (2002)\\nFeast on Scraps (CD/DVD, 2002)\\nSo-Called Chaos (2004)\\nJagged Little Pill Acoustic (2005)\\nAlanis Morissette: The Collection (2005)\\nFlavors of Entanglement (2008)\\nHavoc and Bright Lights (2012)\\n\\nSelected songs\\nMorissette has written many songs. Some of her most famous songs are:\\n\"You Oughta Know\" - This song is to Morissette\\'s ex-boyfriend, a man she once loved. In this song, Morissette is very angry. She wants her ex-boyfriend to know that he caused many problems after leaving her for another woman.\\n\"Ironic\" - This song is about life. It contains several stories about unlucky people. In one of the stories, a man is afraid of flying on airplanes. He finally flies in one, but the airplane crashes.\\n\"You Learn\" - In this song, Morissette says that bad things happen in life, but people learn from them. Anyone can make bad things into good things. She wants people to try new things in life.\\n\"Uninvited\" - In this song, Morissette is not happy because she is famous. She does not know whether she wants to continue to be famous or not.\\n\"Thank U\" - In this song, she thanks many things that have helped her. She thanks India, a country she visited and almost died in. She also lists ways she can improve herself.\\n\"Hands Clean\" - In this song, a man does something bad, and tells Morissette not to tell anyone else the bad thing the man did. She hides the man\\'s secret for many years.\\n\\nReferences\\n\\nOther websites \\n\\n Official website\\n\\n1974 births\\nLiving people\\n \\nAmerican child actors\\nAmerican movie actors\\nAmerican pop musicians\\nAmerican rock singers\\nAmerican singer-songwriters\\nAmerican television actors\\nCanadian movie actors\\nCanadian pop singers\\nCanadian rock singers\\nCanadian singer-songwriters\\nCanadian television actors\\nGrammy Award winners\\nPeople from Ottawa\\nSingers from Ontario\\nTwin people from Canada', 'title': 'Alanis Morissette', 'url': 'https://simple.wikipedia.org/wiki/Alanis%20Morissette'}]\n",
      "1.  Yes, Alanis Morissette won four Grammy Awards for her album Jagged (Distance: 0.157)\n"
     ]
    }
   ],
   "source": [
    "query_result = qna(\"Did Alanis Morissette win a Grammy?\", \"Article\")\n",
    "\n",
    "for i, article in enumerate(query_result):\n",
    "    print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93c4a696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_additional': {'answer': {'endPosition': 218, 'hasAnswer': True, 'property': 'content', 'result': ' Beijing', 'startPosition': 210}, 'distance': 0.13843733}, 'content': 'Beijing is the capital of the People\\'s Republic of China. The city used to be known as Peking.  It is in the northern and eastern parts of the country. It is the world\\'s most populous capital city.\\n\\nThe city of Beijing has played a very important role in the development of China. Many people from different cities and countries come to Beijing to look for better chances to find work. Nearly 15 million people live there. In 2008 Beijing hosted the Summer Olympic Games, and will host the 2022 Winter Olympic Games. It will be the only city to host both.\\n\\nBeijing is well known for its ancient history.  Since the Jin Dynasty, Beijing has been the capital of several dynasties (especially the later ones), including the Yuan, Ming, and Qing. There are many places of historic interest in Beijing.\\n\\nName\\n\\nThe Mandarin Chinese name of the city is Běijīng, which means \"The Northern Capital\". It got this name when the Yongle Emperor of the Ming family of rulers moved most of his government from Nanjing (\"The Southern Capital\") in the early 1400s.  In Chinese, Beijing\\'s name is written  Today, people spell it \"Beijing\" because they use the pinyin way of spelling, which shows what the name should sound like in Mandarin. People used to spell it \"Peking\" because that was the spelling used by some of the first people from Europe to visit the Ming and write home about it; the Jesuits\\' work was made popular by their French brother Du Halde. It then became the official Chinese Postal Map spelling around 1900 and continued to be used until pinyin became more popular.\\n\\nBeijing was also known as Beiping (\"City of Northern Peace\") between 1928 and 1949, when the Nationalists moved the Chinese capital to Nanjing and Chongqing.\\n\\nHistory \\nThe center of Beijing was settled in the 1st millennium BC. In those days, the Kingdom of Yan (燕, Yān) set up their capital where Beijing is today. They called it Ji (蓟, Jì). After the Kingdom of Yan was destroyed, the city became smaller, although it was still an important place.\\n\\nBeijing became more important again in the 10th century, when the Jin dynasty set its capital there. This city was destroyed by Mongol forces in 1215.  Then in 1267, Mongols built a new city on the north side of the Jin capital, and called it \"Great Capital\" (大都, Dàdū), which was the beginning of modern Beijing. When Kublai Khan the Mongolian monarch, set up the Yuan dynasty, this city became his capital.\\n\\nThe Yuan Dynasty, Ming Dynasty and Qing dynasty all made Beijing their capital. When the Qing dynasty lost power and the Republic of China was set up, the new Republic moved its capital from Beijing to Nanjing. When the People\\'s Republic of China seized power, Beijing became the capital of China again.\\n\\nIn 1989, there were protests in Tian\\'anmen Square because some people wanted democracy.\\n\\nSpecial places\\nImportant places in Beijing include:\\n The Great Wall of China (Chángchéng), in the mountains between Beijing and the grasslands of Mongolia\\n The Forbidden City (Gùgōng), the most important home of the emperors of Ming and Qing China\\n Tian\\'anmen Square (Tiān\\'ānmén Guǎngchǎng), surrounded by China\\'s most important government buildings and museums\\n Jingshan & Beihai Parks, the hill overlooking the Forbidden City and the lake beside it, with many temples\\n The Summer Palace (Yìhéyuán) and Old Summer Palace (Yuánmíng Yuán), the more natural home of the last Qing emperors and what is left of an older one\\n Prince Gong\\'s Mansion, a very nice old house for one of the Qing princes\\n The Imperial Ancestral Temple (Tàimiào), where the emperors remembered the earlier people in their families\\n The Temple of Heaven (Tiāntán) and Temple of the Earth (Dìtán), important places for China\\'s old national religion\\n The Temples of the Sun and the Moon, other important places for China\\'s old national religion\\n The Temple of Confucius and Imperial Academy, important places for China\\'s old kind of education\\n Niujie Mosque, a place for Beijing\\'s Muslims and one of the city\\'s oldest buildings\\n The National and Urban Planning Museums\\n Olympic Green, the park left from the 2008 Beijing Olympics\\n Marco Polo Bridge, a very old bridge across the main river west of town\\n Ming Tombs, where many Ming emperors were buried\\n Zhoukoudian, caves in the mountains west of town where people lived long, long ago\\n\\nEducation \\nBeijing is the education center of People\\'s Republic of China. More than 500 famous universities of China are in Beijing.  They also include 5 of the top universities: Peking University, Tsinghua University, China People University, Beijing Normal University, and Beihang University. Beijing is also education center of China for teaching Chinese as a foreign language. The standard Chinese pronunciation is based on Beijing dialect, so over 70% foreigners who want to study Chinese go to Beijing for their studies.\\n\\nSources\\n\\nPages\\n\\nBooks\\n . \\n .\\n\\nOther websites \\n\\n Beijing Travel\\n Beijing Travel Guide\\n Voyage Pékin \\n Photos of Beijing \\n\\n \\nOlympic cities\\nProvinces of China', 'title': 'Beijing', 'url': 'https://simple.wikipedia.org/wiki/Beijing'}]\n",
      "1.  Beijing (Distance: 0.138)\n"
     ]
    }
   ],
   "source": [
    "query_result = qna(\"What is the capital of China?\", \"Article\")\n",
    "\n",
    "for i, article in enumerate(query_result):\n",
    "    if article['_additional']['answer']['hasAnswer'] == False:\n",
    "      print('No answer found')\n",
    "    else:\n",
    "      print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007be48",
   "metadata": {},
   "source": [
    "Thanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
